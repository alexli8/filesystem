# 性能优化

### 小文件优化

实际上前面已经提到，Lustre并不适合小文件I/O应用，性能表现非常差。因此，建议不要将Lustre应用于LOSF场合。不过，Lustre操作手册仍然给出了一些针对小文件的优化措施。

* 通过应用聚合读写提高性能，比如对小文件进行Tar，或创建大文件或通过loopback mount来存储小文件。小文件系统调用开销和额外的I/O开销非常大，应用聚合优化可以显著提高性能。另外，可以使用多节点、多进程/多线程尽可能通过聚合来提高I/O带宽。
* 应用采用O_DIRECT方式进行直接I/O，读写记录大小设置为4KB，与文件系统保持一致。对输出文件禁用locking，避免客户端之间的竞争。
* 应用程序尽量保证写连续数据，顺序读写小文件要明显优于随机小文件I/O。
* OST采用SSD或更多的磁盘，提高IOPS来改善小文件性能。创建大容量OST，而非多个小容量OST，减少日志、连接等负载。
* OST采用RAID 1+0替代RAID 5/6，避免频繁小文件I/O引起的数据校验开销。

Lustre提供了强大的系统监控与控制接口用于进行性能分析与调优，对于小文件I/O，也可以通过调整一些系统参数进行优化。

* 禁用所有客户端LNET debug功能：缺省开启多种调试信息，sysctl -w lnet.debug=0，减少系统开销，但发生错误时将无LOG可询。
* 增加客户端Dirty Cache大小：lctl set_param osc./*.max_dirty_mb=256，缺省为32MB，增大缓存将提升I/O性能，但数据丢失的风险也随之增大。
* 增加RPC并行数量：echo 32 > /proc/fs/lustre/osc/*-OST000*/max_rpcs_in_flight，缺省为8，提升至32将提高数据和元数据性能。不利之处是如果服务器压力很大，可能反而会影响性能。
* 控制Lustre striping：lfs setstripe -c 0/1/-1 /path/filename，如果OST对象数大于1，小文件性能会下降，因此将OST对象设置为1。
* 客户端考虑使用本地锁：mount -t lustre -o localflock，如果确定多个进程从同一个客户端进行写文件，则可用localflock代替flock，减少发送到MDS的RPC数量。
* 使用loopback mount文件：创建大Lustre文件，与loop设备关联并创建文件系统，然后将其作为文件系统进行mount。小文件作用其上，则原先大量的MDS元数据操作将转换为OSS读写操作，消除了元数据瓶颈，可以显著提高小文件性能。这种方法应用于scratch空间可行，但对于生产数据应该谨慎使用，因为Lustre目前工作在这种模式下还存在问题。操作方法如下：

```
 dd if=/dev/zero of=/mnt/lustre/loopback/scratch bs=1048576 count=1024
 losetup /dev/loop0 /mnt/lustre/loopback/scratch
 mkfs -t ext4 /dev/loop0
 mount /dev/loop0 /mnt/losf
 ```

 ### Lustre配置实践

 Lustre具有鲜明的I/O特点，并具有非常高的扩展性和大文件I/O性能。如果进行适当的配置和操作，Lustre则会展现更高的性能。下面给出一些Lustre I/O最佳实践，可根据实际应用情况择优实践。

* 1.使用单进程读取完整的共享小文件，需要时传输数据至其他进程。
* 2.使用单进程访问容量在(1MB, 1GB)之间的小文件，将文件OST对象数设为1。
* 3.使用单进程访问大于1GB的中等文件，文件OST对象数不超过4个。
* 4.远大于1GB的大文件OST对象数应设为＞4，这种文件不要采用顺序I/O或file-per-process的I/O访问模式。
* 5.限制单个目录下的文件数量，包含大量小文件的目录stripe_count设置为1。
* 6.小文件存放在单一OST上，单进程文件创建和读写性能会得到提高。
* 7.包含大量小文件的目录存放在单一OST上，文件创建性能会提到极大提升。
* 8.尽量避免频繁地打开和关闭文件。
* 9.仅在必要时使用ls -l，它会与所有相关的OST交互，操作代价很大，尽可能使用ls和lfs find代替。
* 10.考虑使用I/O中间件来改善性能，如ADIOS、HDF5、MPI-IO。